debug: false
seed: 42
run:
  name: "train_unet_${now:%Y-%m-%d_%H-%M-%S}"
model:
  number_of_input_channels: 16
  number_of_classes: 1
  activation_fn_name: "relu"
  num_encoder_decoder_blocks: 4
  use_batchnorm: true
  optimizer:
    name: "adam"
    params:
      lr : 0.0001
      weight_decay: 0.0001
  lr_scheduler:
    name: "one_cycle_lr"
    params:
      max_lr: 0.0001
      epochs: 50
      steps_per_epoch: 202
output_path: "data/trainings/"
data:
  split_info_file_path: 'data/splits/holdout_split_2010_2023_2024-09-03_17-03-47/data_split_info.json'
  input_data_no_data_value: -32768.0
  input_data_new_no_data_value: 0.0
  target_no_data_value: 127
  data_loading_num_workers: 6
  input_data_indexes_to_remove: []
training:
  max_nb_epochs: 50
  train_batch_size: 16
  eval_batch_size: 32
  loss_name: "dice_loss"
  optimization_metric_name: "dice_loss"
  minimize_optimization_metric: true
  data_augs: []
    # - name: "RandomVerticalFlip"
    #   params:
    #     p: 0.5
    # - name: "RandomHorizontalFlip"
    #   params:
    #     p: 0.5
    # - name: "RandomRotate"
    #   params:
    #     p: 0.5
    #     fill: 0.0
metrics:
  - name: "ce_dice_loss"
    params:
      ce_weight: 0.1
      dice_weight: 0.9
      ce_params: {}
      dice_params:
        smooth: 1e-5
        from_logits: true
  - name: "dice_loss"
    params:
      smooth: 1e-5
      from_logits: true
  - name: "ce_loss"
    params: {}
  - name: "pr_auc"
    params: {}
logging:
  loggers:
    # - loguru
    - cometml